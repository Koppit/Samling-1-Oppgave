{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Sette miljøvariabler for DINOv3\n",
    "# Dette bruker vi nedenfor for å finne nedlastede repoet: Du må skrive din egen sti her\n",
    "\n",
    "# r\"\"\" string is interpeted as a raw string (backslash is literal backslash and not escape characters)\n",
    "\n",
    "os.environ[\"DINOV3_LOCATION\"] = r\"C:\\Users\\Jan Magne\\OneDrive - Akershus fylkeskommune\\dinov3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f645d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sjekke om miljøvariabelen er satt riktig\n",
    "\n",
    "print(\"DINOV3_LOCATION:\", os.getenv(\"DINOV3_LOCATION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df05502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laste ned alle pakkene som trengs: \n",
    "%pip install torch torchvision pillow numpy matplotlib scipy scikit-learn tqdm opencv-python\n",
    "#install er CUDA versjon av torch\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu129\n",
    "# Eller kan man laste ned fra terminalen med \"pip install -r requirements.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69918a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importere nødvendige biblioteker\n",
    "\n",
    "import io\n",
    "import os\n",
    "import pickle\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Kommentere ut denne delen som setter DINOv3_LOCATION\n",
    "\n",
    "# DINOV3_GITHUB_LOCATION = \"facebookresearch/dinov3\"\n",
    "\n",
    "# if os.getenv(\"DINOV3_LOCATION\") is not None:\n",
    "#     DINOV3_LOCATION = os.getenv(\"DINOV3_LOCATION\")\n",
    "# else:\n",
    "#     DINOV3_LOCATION = DINOV3_GITHUB_LOCATION\n",
    "\n",
    "# print(f\"DINOv3 location set to {DINOV3_LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a74abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sett DINOv3 location til miljøvariabelen\n",
    "\n",
    "DINOV3_LOCATION = os.getenv(\"DINOV3_LOCATION\")\n",
    "\n",
    "if DINOV3_LOCATION is None:\n",
    "    raise ValueError(\"DINOV3_LOCATION environment variabel er ikke satt. Se tidligere steg.\")\n",
    "\n",
    "print(\"DINOv3 location set to:\", DINOV3_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d511dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vi starer med denne, da denne er minst og krever mindre ressurser\n",
    "MODEL_DINOV3_VITS = \"dinov3_vits16\"\n",
    "\n",
    "# Andre modeller du kan prøve\n",
    "# Husk at du må laste ned \"weights\" for den modellen du ønsker å bruke\n",
    "MODEL_DINOV3_VITSP = \"dinov3_vits16plus\"\n",
    "MODEL_DINOV3_VITB = \"dinov3_vitb16\"\n",
    "MODEL_DINOV3_VITL = \"dinov3_vitl16\"\n",
    "MODEL_DINOV3_VITHP = \"dinov3_vith16plus\"\n",
    "MODEL_DINOV3_VIT7B = \"dinov3_vit7b16\"\n",
    "\n",
    "# we take DINOv3 ViT-S (since we have the pretrained weights for this model)\n",
    "MODEL_NAME = MODEL_DINOV3_VITHP # Her kan du velge en annen modell \n",
    "\n",
    "# Load model without pretrained weights to avoid web download\n",
    "model = torch.hub.load(\n",
    "    repo_or_dir=DINOV3_LOCATION,\n",
    "    model=MODEL_NAME,\n",
    "    source=\"local\",\n",
    "    pretrained=True \n",
    ")\n",
    "\n",
    "# Set model to evaluation mode and move to GPU\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "# Test the model with a small dummy input to see if it produces valid output\n",
    "print(\"\\nTesting model with dummy input...\")\n",
    "dummy_input = torch.randn(1, 3, 224, 224).cuda()\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        test_output = model(dummy_input)\n",
    "        print(f\"Model test successful. Output shape: {test_output.shape}\")\n",
    "        print(f\"Output has NaN: {torch.isnan(test_output).any()}\")\n",
    "        print(f\"Output range: {test_output.min():.4f} to {test_output.max():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model test failed: {e}\")\n",
    "        print(\"There might be an issue with the model or checkpoint loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba723de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bruke bilder som er lagret lokalt på på \"data\" mappen\n",
    "image_folder = \"./data/foreground_segmentation_images\"\n",
    "label_folder = \"./data/foreground_segmentation_labels\"\n",
    "\n",
    "def load_images_from_folder(folder: str) -> list[Image.Image]:\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img = Image.open(os.path.join(folder, filename))\n",
    "            images.append(img)\n",
    "            print(f\"Loaded {filename} from {folder}\")\n",
    "\n",
    "    print(f\"Total images loaded from {folder}: {len(images)}\\n\")\n",
    "    return images\n",
    "images = load_images_from_folder(image_folder)\n",
    "labels = load_images_from_folder(label_folder)\n",
    "n_images = len(images)\n",
    "\n",
    "assert n_images == len(labels), f\"{len(images)=}, {len(labels)=}\" # Sjekke at vi har like mange bilder og labels\n",
    "\n",
    "print(f\"Loaded {n_images} images and labels from local folders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10618a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_images(amount):\n",
    "    for image, mask in zip(images[:amount], labels[:amount]):\n",
    "        foreground = Image.composite(image, mask, mask)\n",
    "        mask_bg_np = np.copy(np.array(mask))\n",
    "        mask_bg_np[:, :, 3] = 255 - mask_bg_np[:, :, 3]\n",
    "        \n",
    "        mask_bg = Image.fromarray(mask_bg_np)\n",
    "        background = Image.composite(image, mask_bg, mask_bg)\n",
    "\n",
    "        data_to_show = [image, mask, foreground, background]\n",
    "        data_labels = [\"Image\", \"Mask\", \"Foreground\", \"Background\"]\n",
    "\n",
    "        plt.figure(figsize=(16, 4), dpi=300)\n",
    "        \n",
    "        for i in range(len(data_to_show)):\n",
    "            plt.subplot(1, len(data_to_show), i + 1)\n",
    "            plt.imshow(data_to_show[i])\n",
    "            plt.axis('off')\n",
    "            plt.title(data_labels[i], fontsize=12)\n",
    "        plt.show()\n",
    "\n",
    "vise_antall_bilder = 1\n",
    "show_images(vise_antall_bilder)\n",
    "\n",
    "# printe ut et bilde som en numpy array\n",
    "print(np.array(images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konstanter for patch-størrelse og bilde-størrelse\n",
    "\n",
    "PATCH_SIZE = 16    # Hver patch er 16×16 piksler\n",
    "#IMAGE_SIZE = 768   # Standard høyde vi skalerer til (768÷16 = 48 patches høyt)\n",
    "IMAGE_SIZE = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_quant_filter = torch.nn.Conv2d(1, 1, PATCH_SIZE, stride=PATCH_SIZE, bias=False)\n",
    "patch_quant_filter.weight.data.fill_(1.0 / (PATCH_SIZE * PATCH_SIZE)) # Fyller data med 1/(16*16)= 0.00390625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denne funksjonen bruker vi videre for å endre størrelse på maskene slik at de passer med patch-størrelsen\n",
    "def resize_transform(mask_image: Image, image_size: int = IMAGE_SIZE, patch_size: int = PATCH_SIZE) -> torch.Tensor:\n",
    "    w, h = mask_image.size                              # Original størrelse\n",
    "    h_patches = int(image_size / patch_size)            # Antall patches vertikalt (768÷16=48)\n",
    "    w_patches = int((w * image_size) / (h * patch_size)) # Antall patches horisontalt\n",
    "    return TF.to_tensor(TF.resize(mask_image, (h_patches * patch_size, w_patches * patch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69aac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her henter vi en maske og viser original og kvantisert maske\n",
    "# Vi splitter masken i ulike kanaler (RGBA) og bruker bare alpha-kanalen som er siste kanal\n",
    "mask_0 = labels[1].split()[-1] \n",
    "mask_0_resized = resize_transform(mask_0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mask_0_quantized = patch_quant_filter(mask_0_resized).squeeze().detach().cpu()\n",
    "\n",
    "plt.figure(figsize=(4, 2), dpi=300)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(mask_0)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Original Mask, Size {mask_0.size}\", fontsize=5)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask_0_quantized)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Quantized Mask, Size {tuple(mask_0_quantized.shape)}\", fontsize=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "# Plot with grid overlay\n",
    "plt.figure(figsize=(6, 6), dpi=200)\n",
    "plt.imshow(mask_0_resized.squeeze().numpy(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Resized Mask with Patch Grid\")\n",
    "\n",
    "h, w = mask_0_resized.shape[1:]\n",
    "for y in range(0, h, PATCH_SIZE):\n",
    "    for x in range(0, w, PATCH_SIZE):\n",
    "        rect = patches.Rectangle((x, y), PATCH_SIZE, PATCH_SIZE,\n",
    "                                 linewidth=0.3, edgecolor=\"red\", facecolor=\"none\")\n",
    "\n",
    "        # Prosentandel av \"foreground\" i denne patchen        \n",
    "        foreground_present = mask_0_quantized[y // PATCH_SIZE, x // PATCH_SIZE]\n",
    "\n",
    "        # Vise bare tekst hvis det er noe \"foreground\" i patchen\n",
    "        if foreground_present != 0:\n",
    "            rect_text = f\"{foreground_present:.2f}\" # Tar med to desimaler\n",
    "            \n",
    "            # Formatere teksten for å unngå store tall som 0.07 eller 1.00 etc.\n",
    "            # Splitter teksten ved desimalpunktet, vi får tilbake en liste med to elementer for eksempel 0.07 --> [\"0\", \"07\"]\n",
    "            rect_text_split = rect_text.split(\".\")\n",
    "            if rect_text_split[0] == \"0\":\n",
    "                rect_text = f\".{rect_text_split[1]}\"\n",
    "            elif rect_text_split[0] == \"1\":\n",
    "                rect_text = f\"{rect_text_split[0]}\"\n",
    "            else:\n",
    "                rect_text = f\"{rect_text_split[0]}.{rect_text_split[1]}\"\n",
    "\n",
    "            plt.text(x + PATCH_SIZE / 2, y + PATCH_SIZE / 2, rect_text,\n",
    "                    color=\"blue\", fontsize=3, ha=\"center\", va=\"center\")\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vi kan printe litt informasjon om modellen\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "image_index = []\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406) # RGB mean for ImageNet\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225) # RGB std for ImageNet\n",
    "\n",
    "# Self-attention lagene i DINOv3 ViT modellene\n",
    "MODEL_TO_NUM_LAYERS = {\n",
    "    MODEL_DINOV3_VITS: 12,\n",
    "    MODEL_DINOV3_VITSP: 12,\n",
    "    MODEL_DINOV3_VITB: 12,\n",
    "    MODEL_DINOV3_VITL: 24,\n",
    "    MODEL_DINOV3_VITHP: 32,\n",
    "    MODEL_DINOV3_VIT7B: 40,\n",
    "}\n",
    "\n",
    "n_layers = MODEL_TO_NUM_LAYERS[MODEL_NAME]\n",
    "\n",
    "# \n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
    "        for i in tqdm(range(n_images), desc=\"Processing images\"):\n",
    "\n",
    "            # Loading the ground truth\n",
    "            mask_i = labels[i].split()[-1]\n",
    "            mask_i_resized = resize_transform(mask_i)\n",
    "            mask_i_quantized = patch_quant_filter(mask_i_resized).squeeze().view(-1).detach().cpu()\n",
    "            ys.append(mask_i_quantized)\n",
    "            \n",
    "            # Loading the image data \n",
    "            image_i = images[i].convert('RGB')\n",
    "            image_i_resized = resize_transform(image_i)\n",
    "            image_i_resized = TF.normalize(image_i_resized, mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "            image_i_resized = image_i_resized.unsqueeze(0).cuda()\n",
    "\n",
    "            # tar ut features fra alle lag i modellen\n",
    "            # feats blir en liste med feature tensorer fra hvert lag\n",
    "            # Forklaring av tensor: \n",
    "            # en tensor er (N, D, H, W) der N=1 (batch size), D=dimensjon på features, H og W er romlige dimensjoner\n",
    "            feats = model.get_intermediate_layers(image_i_resized, n=range(n_layers), reshape=True, norm=True)\n",
    "            \n",
    "            print(f\"Feature map shape: {len(feats)}, type: {type(feats)}\")\n",
    "            print(f\"Siste tensor shape: {[feats[-1].shape]}\")\n",
    "\n",
    "            # tar med features fra siste lag\n",
    "            # Vi endrer formen på tensoren slik at vi får en rad per patch\n",
    "            # feats[-1] er siste lag, som har formen (1, D, H, W)\n",
    "            dim = feats[-1].shape[1]\n",
    "\n",
    "            print(f\"dim = {dim}\")\n",
    "            print(f\"Feature map shape: {feats[0].shape}\")\n",
    "            print(f\"feature fra siste lag: {feats[-1]}\")\n",
    "\n",
    "            xs.append(feats[-1].squeeze().view(dim, -1).permute(1,0).detach().cpu())\n",
    "\n",
    "            image_index.append(i * torch.ones(ys[-1].shape))\n",
    "\n",
    "             # Visualization of ground truth, predicted mask, input image\n",
    "            plt.figure(figsize=(12, 6), dpi=200)\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(labels[i])\n",
    "            plt.title(\"Ground Truth\")\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(mask_i)\n",
    "            plt.title(\"Predicted Mask\")\n",
    "\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(image_i_resized.squeeze().permute(1,2,0).detach().cpu())\n",
    "            plt.title(\"Input Image\")\n",
    "    \n",
    "            plt.show()\n",
    "\n",
    "            print(xs[i].shape)\n",
    "\n",
    "# Concatenate all lists into torch tensors \n",
    "xs = torch.cat(xs)\n",
    "ys = torch.cat(ys)\n",
    "image_index = torch.cat(image_index)\n",
    "\n",
    "# keeping only the patches that have clear positive or negative label\n",
    "idx = (ys < 0.01) | (ys > 0.99)\n",
    "xs = xs[idx]\n",
    "ys = ys[idx]\n",
    "image_index = image_index[idx]\n",
    "\n",
    "print(\"Design matrix of size : \", xs.shape)\n",
    "print(\"Label matrix of size : \", ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = np.logspace(-7, 0, 8)\n",
    "scores = np.zeros((n_images, len(cs)))\n",
    "\n",
    "for i in range(n_images):\n",
    "    # We use leave-one-out so train will be all but image i, val will be image i\n",
    "    print('validation using image_{:02d}.jpg'.format(i+1))\n",
    "    train_selection = image_index != float(i)\n",
    "    fold_x = xs[train_selection].numpy()\n",
    "    fold_y = (ys[train_selection] > 0).long().numpy()\n",
    "    val_x = xs[~train_selection].numpy()\n",
    "    val_y = (ys[~train_selection] > 0).long().numpy()\n",
    "\n",
    "    plt.figure()\n",
    "    for j, c in enumerate(cs):\n",
    "        print(\"training logisitic regression with C={:.2e}\".format(c))\n",
    "        clf = LogisticRegression(random_state=0, C=c, max_iter=10000).fit(fold_x, fold_y)\n",
    "        output = clf.predict_proba(val_x)\n",
    "        precision, recall, thresholds = precision_recall_curve(val_y, output[:, 1])\n",
    "        s = average_precision_score(val_y, output[:, 1])\n",
    "        scores[i, j] = s\n",
    "        plt.plot(recall, precision, label='C={:.1e} AP={:.1f}'.format(c, s*100))\n",
    "\n",
    "    plt.grid()\n",
    "    plt.xlabel('recall')\n",
    "    plt.title('image_{:02d}.jpg'.format(i+1))\n",
    "    plt.ylabel('precision')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685209ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(scores))\n",
    "print(scores.shape)\n",
    "print(scores)\n",
    "\n",
    "print(np.max(scores, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2), dpi=300)\n",
    "plt.rcParams.update({\n",
    "    \"xtick.labelsize\": 5,\n",
    "    \"ytick.labelsize\": 5,\n",
    "    \"axes.labelsize\": 5,\n",
    "})\n",
    "\n",
    "print(c)\n",
    "plt.plot(scores.mean(axis=0))\n",
    "plt.xticks(np.arange(len(cs)), [\"{:.0e}\".format(c) for c in cs])\n",
    "plt.xlabel('data fit C')\n",
    "plt.ylabel('average AP')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcfd21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, C=1, max_iter=100000, verbose=2).fit(xs.numpy(), (ys > 0).long().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ed685",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r\"./data/test_images/\"\n",
    "\n",
    "for filename in sorted(os.listdir(folder)):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "\n",
    "            test_image = Image.open(os.path.join(folder, filename))\n",
    "            test_image_resized = resize_transform(test_image)\n",
    "            test_image_normalized = TF.normalize(test_image_resized, mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
    "                    feats = model.get_intermediate_layers(test_image_normalized.unsqueeze(0).cuda(), n=range(n_layers), reshape=True, norm=True)\n",
    "                    x = feats[-1].squeeze().detach().cpu()\n",
    "                    dim = x.shape[0]\n",
    "                    x = x.view(dim, -1).permute(1, 0)\n",
    "\n",
    "            h_patches, w_patches = [int(d / PATCH_SIZE) for d in test_image_resized.shape[1:]]\n",
    "\n",
    "            fg_score = clf.predict_proba(x)[:, 1].reshape(h_patches, w_patches)\n",
    "            fg_score_mf = torch.from_numpy(signal.medfilt2d(fg_score, kernel_size=3))\n",
    "\n",
    "            plt.figure(figsize=(9, 3), dpi=300)\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(test_image_resized.permute(1, 2, 0))\n",
    "            plt.title('input image')\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(fg_score)\n",
    "            plt.title('foreground score')\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(fg_score_mf)\n",
    "            plt.title('+ median filter')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root = '.'\n",
    "model_path = os.path.join(save_root, \"fg_classifier.pkl\")\n",
    "with open(model_path, \"wb\") as f:\n",
    "  pickle.dump(clf, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
