{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdfb969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DINOV3_LOCATION\"] = r\"C:\\Users\\Jan Magne\\OneDrive - Akershus fylkeskommune\\dinov3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f645d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOV3_LOCATION: C:\\Users\\Jan Magne\\OneDrive - Akershus fylkeskommune\\dinov3\n"
     ]
    }
   ],
   "source": [
    "# Sjekke om miljøvariabelen er satt riktig\n",
    "\n",
    "print(\"DINOV3_LOCATION:\", os.getenv(\"DINOV3_LOCATION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69918a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importere nødvendige biblioteker\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a74abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv3 location set to: C:\\Users\\Jan Magne\\OneDrive - Akershus fylkeskommune\\dinov3\n"
     ]
    }
   ],
   "source": [
    "# sett DINOv3 location til miljøvariabelen\n",
    "\n",
    "DINOV3_LOCATION = os.getenv(\"DINOV3_LOCATION\")\n",
    "\n",
    "if DINOV3_LOCATION is None:\n",
    "    raise ValueError(\"DINOV3_LOCATION environment variabel er ikke satt. Se tidligere steg.\")\n",
    "\n",
    "print(\"DINOv3 location set to:\", DINOV3_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9877d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu129\n",
      "True\n",
      "12.9\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d511dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model with dummy input...\n",
      "Model test successful. Output shape: torch.Size([1, 768])\n",
      "Output has NaN: False\n",
      "Output range: -2.1267 to 2.0255\n"
     ]
    }
   ],
   "source": [
    "# Vi starer med denne, da denne er minst og krever mindre ressurser\n",
    "MODEL_DINOV3_VITS = \"dinov3_vits16\"\n",
    "\n",
    "# Andre modeller du kan prøve\n",
    "# Husk at du må laste ned \"weights\" for den modellen du ønsker å bruke\n",
    "MODEL_DINOV3_VITSP = \"dinov3_vits16plus\"\n",
    "MODEL_DINOV3_VITB = \"dinov3_vitb16\"\n",
    "MODEL_DINOV3_VITL = \"dinov3_vitl16\"\n",
    "MODEL_DINOV3_VITHP = \"dinov3_vith16plus\"\n",
    "MODEL_DINOV3_VIT7B = \"dinov3_vit7b16\"\n",
    "\n",
    "# we take DINOv3 ViT-S (since we have the pretrained weights for this model)\n",
    "MODEL_NAME = MODEL_DINOV3_VITB # Her kan du velge en annen modell \n",
    "\n",
    "# Load model without pretrained weights to avoid web download\n",
    "model = torch.hub.load(\n",
    "    repo_or_dir=DINOV3_LOCATION,\n",
    "    model=MODEL_NAME,\n",
    "    source=\"local\",\n",
    "    pretrained=True \n",
    ")\n",
    "\n",
    "# Set model to evaluation mode and move to GPU\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "# Test the model with a small dummy input to see if it produces valid output\n",
    "print(\"\\nTesting model with dummy input...\")\n",
    "dummy_input = torch.randn(1, 3, 224, 224).cuda()\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        test_output = model(dummy_input)\n",
    "        print(f\"Model test successful. Output shape: {test_output.shape}\")\n",
    "        print(f\"Output has NaN: {torch.isnan(test_output).any()}\")\n",
    "        print(f\"Output range: {test_output.min():.4f} to {test_output.max():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model test failed: {e}\")\n",
    "        print(\"There might be an issue with the model or checkpoint loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "111c8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konstanter for patch-størrelse og bilde-størrelse\n",
    "\n",
    "PATCH_SIZE = 16    # Hver patch er 16×16 piksler\n",
    "#IMAGE_SIZE = 768   # Standard høyde vi skalerer til (768÷16 = 48 patches høyt)\n",
    "IMAGE_SIZE = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b981391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denne funksjonen bruker vi videre for å endre størrelse på maskene slik at de passer med patch-størrelsen\n",
    "def resize_transform(mask_image: Image, image_size: int = IMAGE_SIZE, patch_size: int = PATCH_SIZE) -> torch.Tensor:\n",
    "    w, h = mask_image.size                              # Original størrelse\n",
    "    h_patches = int(image_size / patch_size)            # Antall patches vertikalt (768÷16=48)\n",
    "    w_patches = int((w * image_size) / (h * patch_size)) # Antall patches horisontalt\n",
    "    return TF.to_tensor(TF.resize(mask_image, (h_patches * patch_size, w_patches * patch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bcfd21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('fg_classifier_VITB.pkl', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "# Self-attention lagene i DINOv3 ViT modellene\n",
    "MODEL_TO_NUM_LAYERS = {\n",
    "    MODEL_DINOV3_VITS: 12,\n",
    "    MODEL_DINOV3_VITSP: 12,\n",
    "    MODEL_DINOV3_VITB: 12,\n",
    "    MODEL_DINOV3_VITL: 24,\n",
    "    MODEL_DINOV3_VITHP: 32,\n",
    "    MODEL_DINOV3_VIT7B: 40,\n",
    "}\n",
    "\n",
    "n_layers = MODEL_TO_NUM_LAYERS[MODEL_NAME]\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406) # RGB mean for ImageNet\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225) # RGB std for ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be41ce63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: -1 frames at 30.0 FPS\n",
      "Controls: 'q' to quit, 'space' to pause/resume, 's' to save frame\n",
      "Quitting...\n",
      "Video display ended\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def show_video_with_foreground_mask(model, clf, resize_transform, \n",
    "                                   IMAGENET_MEAN, IMAGENET_STD, PATCH_SIZE, n_layers,\n",
    "                                   apply_median_filter=True, show_original=True, \n",
    "                                   fps_limit=30, window_size=(1200, 600)):\n",
    "\n",
    "    \n",
    "    def process_frame_for_segmentation(frame):\n",
    "        \"\"\"Process a single frame and return foreground mask\"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_pil = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        # Preprocess\n",
    "        frame_resized = resize_transform(frame_pil)\n",
    "        frame_normalized = TF.normalize(frame_resized, mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.inference_mode():\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
    "                feats = model.get_intermediate_layers(\n",
    "                    frame_normalized.unsqueeze(0).cuda(), \n",
    "                    n=range(n_layers), \n",
    "                    reshape=True, \n",
    "                    norm=True\n",
    "                )\n",
    "                x = feats[-1].squeeze().detach().cpu()\n",
    "                dim = x.shape[0]\n",
    "                x = x.view(dim, -1).permute(1, 0)\n",
    "        \n",
    "        # Get foreground scores\n",
    "        h_patches, w_patches = [int(d / PATCH_SIZE) for d in frame_resized.shape[1:]]\n",
    "        fg_score = clf.predict_proba(x)[:, 1].reshape(h_patches, w_patches)\n",
    "        \n",
    "        # Apply median filter if requested\n",
    "        if apply_median_filter:\n",
    "            fg_score = signal.medfilt2d(fg_score, kernel_size=3)\n",
    "        \n",
    "        return fg_score, frame_resized\n",
    "    \n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    print(f\"Video: {total_frames} frames at {original_fps:.1f} FPS\")\n",
    "    print(\"Controls: 'q' to quit, 'space' to pause/resume, 's' to save frame\")\n",
    "    \n",
    "    # Calculate frame delay for FPS limiting\n",
    "    frame_delay = max(1, int(1000 / min(fps_limit, original_fps)))\n",
    "    \n",
    "    paused = False\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        if not paused:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"End of video reached\")\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "        \n",
    "        try:\n",
    "            # Process frame for segmentation\n",
    "            fg_score, frame_resized = process_frame_for_segmentation(frame)\n",
    "            \n",
    "            # Convert tensors to numpy for display\n",
    "            frame_np = (frame_resized.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "            \n",
    "            # Create colorized mask\n",
    "            fg_score_norm = (fg_score * 255).astype(np.uint8)\n",
    "            mask_colored = cv2.applyColorMap(fg_score_norm, cv2.COLORMAP_HOT)\n",
    "            \n",
    "            # Resize mask to match frame size\n",
    "            frame_height, frame_width = frame_np.shape[:2]\n",
    "            mask_resized = cv2.resize(mask_colored, (frame_width, frame_height))\n",
    "            \n",
    "            if show_original:\n",
    "                # Show original and mask side by side\n",
    "                frame_bgr = cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR)\n",
    "                combined = np.hstack([frame_bgr, mask_resized])\n",
    "                \n",
    "                # Add text overlay\n",
    "                cv2.putText(combined, f\"Frame: {frame_count}/{total_frames}\", \n",
    "                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                cv2.putText(combined, \"Original\", (10, frame_height - 10), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                cv2.putText(combined, \"Foreground Mask\", (frame_width + 10, frame_height - 10), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                \n",
    "                display_frame = combined\n",
    "            else:\n",
    "                # Show only the mask\n",
    "                cv2.putText(mask_resized, f\"Frame: {frame_count}/{total_frames}\", \n",
    "                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                display_frame = mask_resized\n",
    "            \n",
    "            # Resize for display if needed\n",
    "            display_height, display_width = display_frame.shape[:2]\n",
    "            if display_width > window_size[0] or display_height > window_size[1]:\n",
    "                scale = min(window_size[0] / display_width, window_size[1] / display_height)\n",
    "                new_width = int(display_width * scale)\n",
    "                new_height = int(display_height * scale)\n",
    "                display_frame = cv2.resize(display_frame, (new_width, new_height))\n",
    "            \n",
    "            # Show frame\n",
    "            cv2.imshow('Foreground Segmentation', display_frame)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {frame_count}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Handle keyboard input\n",
    "        key = cv2.waitKey(frame_delay) & 0xFF\n",
    "        \n",
    "        if key == ord('q'):\n",
    "            print(\"Quitting...\")\n",
    "            break\n",
    "        elif key == ord(' '):  # Space bar\n",
    "            paused = not paused\n",
    "            print(\"Paused\" if paused else \"Resumed\")\n",
    "        elif key == ord('s'):  # Save frame\n",
    "            filename = f\"frame_{frame_count:06d}_segmentation.png\"\n",
    "            cv2.imwrite(filename, display_frame)\n",
    "            print(f\"Saved {filename}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Video display ended\")\n",
    "\n",
    "\n",
    "show_video_with_foreground_mask(\n",
    "    model, clf, resize_transform,\n",
    "    IMAGENET_MEAN, IMAGENET_STD, PATCH_SIZE, n_layers\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
